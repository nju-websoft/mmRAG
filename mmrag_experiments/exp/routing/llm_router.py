import llm
import json
import tqdm
from huggingface_hub import HfApi

NQ_info = '''

Dataset Summary
The NQ corpus contains questions from real users, and it requires QA systems to read and comprehend an entire Wikipedia article that may or may not contain the answer to the question. The inclusion of real user questions, and the requirement that solutions should read an entire page to find the answer, cause NQ to be a more realistic and challenging task than prior QA datasets.

Supported Tasks and Leaderboards
https://ai.google.com/research/NaturalQuestions

Data Format
Text

Languages
en
'''

Triviaqa = '''
Dataset Card for "trivia_qa"
Dataset Summary
TriviaqQA is a reading comprehension dataset containing over 650K question-answer-evidence triples. TriviaqQA includes 95K question-answer pairs authored by trivia enthusiasts and independently gathered evidence documents, six per question on average, that provide high quality distant supervision for answering the questions.

Supported Tasks and Leaderboards
More Information Needed

Data Format
Text

Languages
English.
'''

OTT = '''
This respository contains the OTT-QA dataset used in Open Question Answering over Tables and Text published in ICLR2021 and the baseline code for the dataset OTT-QA. This dataset contains open questions which require retrieving tables and text from the web to answer. This dataset is re-annotated from the previous HybridQA dataset.
What's new compared to HybridQA:

The questions are de-contextualized to be standalone without relying on the given context to understand.
We add new dev/test set questions the newly crawled tables, which removes the potential bias in table retrieval.
The groundtruth table and passage are not given to the model, it needs to retrieve from 400K+ candidates of tables and 5M candidates of passages to find the evidence.
The tables in OTT-QA do not have groundtruth hyperlinks, which simulates a more general scenario outside Wikipedia.

Data Format
Text+Table
'''

TAT = '''
TAT-QA (Tabular And Textual dataset for Question Answering) is a large-scale QA dataset, aiming to stimulate progress of QA research over more complex and realistic tabular and textual data, especially those requiring numerical reasoning.

The unique features of TAT-QA include:

The context given is hybrid, comprising a semi-structured table and at least two relevant paragraphs that describe, analyze or complement the table;
The questions are generated by the humans with rich financial knowledge, most are practical;
The answer forms are diverse, including single span, multiple spans and free-form;
To answer the questions, various numerical reasoning capabilities are usually required, including addition (+), subtraction (-), multiplication (x), division (/), counting, comparison, sorting, and their compositions;
In addition to the ground-truth answers, the corresponding derivations and scale are also provided if any.
In total, TAT-QA contains 16,552 questions associated with 2,757 hybrid contexts from real-world financial reports.

For more details, please refer to the project page: https://nextplusplus.github.io/TAT-QA/

Data Format
Text+Table
'''

CWQ = '''
A dataset for answering complex questions that require reasoning over multiple web snippets.
ComplexWebQuestions is a new dataset that contains a large set of complex questions in natural language, and can be used in multiple ways:

By interacting with a search engine, which is the focus of our paper (Talmor and Berant, 2018);

As a reading comprehension task: we release 12,725,989 web snippets that are relevant for the questions, and were collected during the development of our model; 

As a semantic parsing task: each question is paired with a SPARQL query that can be executed against Freebase to retrieve the answer.

Data Format
Knowledge Graph

'''

WEBQSP = '''
The WebQuestionsSP dataset is released as part of our ACL-2016 paper “The Value of Semantic Parse Labeling for Knowledge Base Question Answering” [Yih, Richardson, Meek, Chang & Suh, 2016], in which we evaluated the value of gathering semantic parses, vs. answers, for a set of questions that originally comes from WebQuestions [Berant et al., 2013]. The WebQuestionsSP dataset contains full semantic parses in SPARQL queries for 4,737 questions, and “partial” annotations for the remaining 1,073 questions for which a valid parse could not be formulated or where the question itself is bad or needs a descriptive answer. This release also includes an evaluation script and the output of the STAGG semantic parsing system when trained using the full semantic parses. More detail can be found in the document and labeling instructions included in this release, as well as the paper.

Data Format
Knowledge Graph

'''

prompt = '''
Classify the following query into some of seven categories: [No, NQ, TriviaQA. OTT, TAT, CWQ, WebQSP], based on whether it requires retrieval-augmented generation (RAG) and the most
appropriate dataset. Consider:
• No: The query can be answered directly with common knowledge, reasoning, or computation
without external data.
• NQ: {}
• TriviaQA: {}
• OTT: {}
• TAT: {}
• CWQ: {}
• WebQSP: {}

You should choose the most possible datasets, consider the data format and features.

Classify the following query: {}
Provide only the categories in list format, rank by the possibility of the query can be answered by the datasource, for example,example1:['NQ', 'TriviaQA'] means NQ is the most relevant, then Triviaqa. example2:['CWQ']. example3:['No']. Notice that No can only appear singly.
'''

# prompt = '''
# Classify the following query into some of three categories: [No, Text, Table, KG], based on whether it requires retrieval-augmented generation (RAG) and the most
# appropriate modality. Consider:
# • No: The query can be answered directly with common knowledge, reasoning, or computation
# without external data.
# • Text: The query requires retrieving straightforward explanations,
# or concise summaries from a single source. Text datasets are NQ and Triviaqa
# • Table: The query requires retrieving tabular data, such as numerical values, structured information. Table datasets are OTT-QA and TAT-QA.
# • KG: The query requires retrieving knowledge graph data, KG provide more information about relationships between entities. KG datasets Freebase.
#
# You should choose the most possible modalities. If the query can be answered better with KG or Table, choose it rather than text.
# Classify the following query: {}
# Provide only the categories in list format, for example,example1:['Text', 'Table'] example2:['KG']. example3:['No']. Notice that No can only appear singly.
# '''


# 实验：应用大模型对query进行分类
glm = llm.GLM('glm-4-plus')

with open("../data/mmrag_ds_test12.json", 'r', encoding='utf-8') as f:
    queries = json.load(f)
results = {}
for query in tqdm.tqdm(queries):
    question = query["query"]
    input_prompt = prompt.format(NQ_info, Triviaqa, OTT, TAT, CWQ, WEBQSP, question)
    result = glm.generate(input_prompt)
    # print(f"Input: {input_prompt}\nResult: {result}")
    # 尝试读取为列表
    try:
        result = eval(str(result))
    except:
        pass
    if not isinstance(result, list):
        result = ["No"]
    results[query["id"]] = result
    # print(f"Query: {question}\nResult: {result}\nAnswer: {query['dataset_score']}")
# 保存结果
with open("llm_router_result_ranked.json", 'w', encoding='utf-8') as f:
    json.dump(results, f, ensure_ascii=False, indent=4)

